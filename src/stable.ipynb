{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ca215b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import json \n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import motornet as mn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import CCA\n",
    "import math\n",
    "from torch.distributions import Normal\n",
    "from gymnasium import Wrapper\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.callbacks import ProgressBarCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329fb955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamics_analysis import *\n",
    "from environment import *\n",
    "from networks import *\n",
    "from neural_activity import *\n",
    "from utils import *\n",
    "from training import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73005638",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b2151c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1LossCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0, log_path=\"./logs/\"):\n",
    "        super().__init__(verbose)\n",
    "        self.log_path = log_path\n",
    "        self.losses = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Info dict for each parallel env\n",
    "        infos = self.locals.get(\"infos\", [])\n",
    "\n",
    "        for info in infos:\n",
    "            if 'goal' in info and 'fingertip' in info:\n",
    "                goal = np.array(info['goal'])\n",
    "                fingertip = np.array(info['fingertip'])\n",
    "\n",
    "                l1_loss = np.abs(goal - fingertip).sum()\n",
    "                self.losses.append(l1_loss)\n",
    "\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"L1 loss: {l1_loss:.4f}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        # Optional: Save the loss values for later\n",
    "        os.makedirs(self.log_path, exist_ok=True)\n",
    "        np.save(os.path.join(self.log_path, \"l1_losses.npy\"), np.array(self.losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d306b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotorNetBatchLoadWrapper(Wrapper):\n",
    "    \"\"\"\n",
    "    Ensures actions have shape (batch, n_muscles) and supplies\n",
    "    default zero loads for endpoint and joints on every step().\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: mn.environment.RandomTargetReach):\n",
    "        super().__init__(env)\n",
    "        # Probe once to figure out load shapes:\n",
    "        obs, info = env.reset(options={\"batch_size\": 1})\n",
    "        geom_state = info[\"states\"][\"geometry\"]  # (1, n_points, 2)\n",
    "        joint_state = info[\"states\"][\"joint\"]    # (1, n_joints)\n",
    "\n",
    "        # For endpoint_load we need one value per coordinate axis (x,y)\n",
    "        self.endpoint_dim = geom_state.shape[2]  # = 2\n",
    "        # For joint_load we need one per joint\n",
    "        self.n_joints = info[\"states\"][\"joint\"].shape[1] // 2\n",
    "        print(f\"Detected: endpoint_dim={self.endpoint_dim}, n_joints={self.n_joints}, n_muscles={self.n_muscles}\")\n",
    "        # For actions we need one per muscle\n",
    "        self.n_muscles   = env.n_muscles\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.array(action, dtype=np.float32)\n",
    "        if action.ndim == 1:\n",
    "            action = action[None, :]  # shape (1, act_dim)\n",
    "\n",
    "        batch = action.shape[0]\n",
    "\n",
    "        endpoint_load = np.zeros((batch, self.endpoint_dim), dtype=np.float32)\n",
    "        joint_load = np.zeros((batch, self.n_joints), dtype=np.float32)\n",
    "\n",
    "        obs, _, terminated, truncated, info = self.env.step(\n",
    "            action=action,\n",
    "            endpoint_load=endpoint_load,\n",
    "            joint_load=joint_load\n",
    "        )\n",
    "\n",
    "        target = info[\"goal\"]\n",
    "        fingertip = info[\"states\"][\"fingertip\"]\n",
    "        reward = -np.linalg.norm(fingertip - target, axis=1)  # shape (batch,)\n",
    "\n",
    "        # Use the first sample\n",
    "        obs = obs[0]\n",
    "        reward = float(reward[0])\n",
    "        terminated = bool(terminated)\n",
    "        truncated = bool(truncated)\n",
    "        info = {k: (v[0] if isinstance(v, np.ndarray) and v.shape[0] == batch else v) for k, v in info.items()}\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomACPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, observation_space, action_space, lr_schedule, **kwargs):\n",
    "        device = kwargs.get('device', 'cpu')\n",
    "        kwargs.pop('device')\n",
    "        super().__init__(observation_space, action_space, lr_schedule, **kwargs)\n",
    "        self._build_network(device)\n",
    "        self._hidden_states = None\n",
    "\n",
    "    def _build_network(self, device):\n",
    "        input_dim = self.observation_space.shape[0]\n",
    "        self.action_dim = self.action_space.shape[0]\n",
    "        self.hidden_dim = 64  # Match your GRU hidden size\n",
    "\n",
    "        # Actor (GRU-based) and Critic\n",
    "        self.actor = Policy(input_dim, self.hidden_dim, self.action_dim, device)\n",
    "        self.critic = Critic(input_dim, device)\n",
    "\n",
    "    def reset_hidden(self, batch_size=1):\n",
    "        self._hidden_states = self.actor.init_hidden(batch_size)\n",
    "\n",
    "    def forward(self, obs, deterministic=False):\n",
    "        if self._hidden_states is None or self._hidden_states.size(1) != obs.shape[0]:\n",
    "            self.reset_hidden(obs.shape[0])\n",
    "        \n",
    "        mean, new_hidden = self.actor(obs, self._hidden_states)\n",
    "        self._hidden_states = new_hidden.detach()\n",
    "\n",
    "        # Define action distribution\n",
    "        std = th.ones_like(mean) * 0.1  # or learnable\n",
    "        dist = th.distributions.Normal(mean, std)\n",
    "\n",
    "        # Sample action\n",
    "        actions = dist.mean if deterministic else dist.sample()\n",
    "        log_probs = dist.log_prob(actions).sum(dim=-1, keepdim=True)\n",
    "\n",
    "        values = self.critic(obs)\n",
    "\n",
    "        return actions, values, log_probs\n",
    "\n",
    "    def evaluate_actions(self, obs, actions):\n",
    "        if self._hidden_states is None or self._hidden_states.size(1) != obs.shape[0]:\n",
    "            self.reset_hidden(obs.shape[0])\n",
    "\n",
    "        mean, new_hidden = self.actor(obs, self._hidden_states)\n",
    "        self._hidden_states = new_hidden.detach()\n",
    "\n",
    "        std = th.ones_like(mean) * 0.1\n",
    "        dist = th.distributions.Normal(mean, std)\n",
    "\n",
    "        log_probs = dist.log_prob(actions).sum(dim=-1, keepdim=True)\n",
    "        entropy = dist.entropy().sum(dim=-1, keepdim=True)\n",
    "        values = self.critic(obs)\n",
    "\n",
    "        if values.ndim == 1:\n",
    "            values = values.unsqueeze(1)\n",
    "\n",
    "        return values, log_probs, entropy\n",
    "    \n",
    "    \n",
    "def PPO_train():\n",
    "    # Create the MotorNet environment\n",
    "    effector = mn.effector.RigidTendonArm26(muscle=mn.muscle.MujocoHillMuscle())\n",
    "    env = mn.environment.RandomTargetReach(effector=effector, max_ep_duration=5.)\n",
    "\n",
    "    # 2. Wrap it\n",
    "    wrapped_env = MotorNetBatchLoadWrapper(env)\n",
    "\n",
    "    #sanity check\n",
    "    print(\"Wrapped action_space:\", wrapped_env.action_space)\n",
    "    print(\"Wrapped observation_space:\", wrapped_env.observation_space)\n",
    "    \n",
    "    # Instantiate the custom PPO model\n",
    "    model = PPO(\n",
    "        CustomACPolicy,\n",
    "        wrapped_env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=10000,  # Adjust based on your environment's requirements\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        verbose=1,\n",
    "        policy_kwargs={\n",
    "            'device': 'cuda:0' if th.cuda.is_available() else 'cpu'\n",
    "        }\n",
    "    )\n",
    "    callback = L1LossCallback(verbose=1)\n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=1000, callback=callback)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1cdcfaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected: endpoint_dim=6, n_joints=2, n_muscles=6\n",
      "Wrapped action_space: Box(0.0, 1.0, (6,), float32)\n",
      "Wrapped observation_space: Box(-inf, inf, (16,), float32)\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 501      |\n",
      "|    ep_rew_mean     | -180     |\n",
      "| time/              |          |\n",
      "|    fps             | 160      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_model = PPO_train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e9f92c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pretrained(policy, env, batch_size):\n",
    "    \"\"\"Evaluation function with hidden state management\"\"\"\n",
    "    # Reset hidden states\n",
    "    policy.hidden_states = policy.actor.init_hidden(batch_size)\n",
    "    \n",
    "    # Initialize environment\n",
    "    obs, info = env.reset(options={\"batch_size\": batch_size})\n",
    "    terminated = np.zeros(batch_size, dtype=bool)\n",
    "    xy = [info[\"states\"][\"fingertip\"][:, None, :]]\n",
    "    tg = [info[\"goal\"][:, None, :]]\n",
    "\n",
    "    # Run evaluation episode\n",
    "    while not terminated.all():\n",
    "        action, _, _, _ = policy(obs, deterministic=True)  # Deterministic actions\n",
    "        obs, _, terminated, _, info = env.step(action)\n",
    "        xy.append(info[\"states\"][\"fingertip\"][:, None, :])\n",
    "        tg.append(info[\"goal\"][:, None, :])\n",
    "\n",
    "    # Plot results\n",
    "    xy = th.cat(xy, axis=1).detach().numpy()\n",
    "    tg = th.cat(tg, axis=1).detach().numpy()\n",
    "    plot_simulations(xy=xy, target_xy=tg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1aba1285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected: endpoint_dim=6, n_joints=2, n_muscles=6\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input and hidden tensors are not at the same device, found input tensor at cpu and hidden tensor at cuda:0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m device = th.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m trained_model.policy.to(device)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mevaluate_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrapped_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mevaluate_pretrained\u001b[39m\u001b[34m(policy, env, batch_size)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Run evaluation episode\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminated.all():\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     action, _, _, _ = \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Deterministic actions\u001b[39;00m\n\u001b[32m     15\u001b[39m     obs, _, terminated, _, info = env.step(action)\n\u001b[32m     16\u001b[39m     xy.append(info[\u001b[33m\"\u001b[39m\u001b[33mstates\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mfingertip\u001b[39m\u001b[33m\"\u001b[39m][:, \u001b[38;5;28;01mNone\u001b[39;00m, :])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mCustomACPolicy.forward\u001b[39m\u001b[34m(self, obs, deterministic)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._hidden_states.size(\u001b[32m1\u001b[39m) != obs.shape[\u001b[32m0\u001b[39m]:\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mself\u001b[39m.reset_hidden(obs.shape[\u001b[32m0\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m mean, new_hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_hidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mself\u001b[39m._hidden_states = new_hidden.detach()\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Define action distribution\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\OneDrive\\Documents\\Udem\\Winter 2025\\MAT 6215\\Brain-like-RL-Dynamics\\src\\networks.py:48\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(self, x, h0)\u001b[39m\n\u001b[32m     44\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected parameter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mself\u001b[39m.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, h0):\n\u001b[32m     49\u001b[39m     y, h = \u001b[38;5;28mself\u001b[39m.gru(x.unsqueeze(\u001b[32m1\u001b[39m), h0)  \n\u001b[32m     50\u001b[39m     u = \u001b[38;5;28mself\u001b[39m.sigmoid(\u001b[38;5;28mself\u001b[39m.fc(y.squeeze(\u001b[32m1\u001b[39m))) \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1100\u001b[39m, in \u001b[36mGRU.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1098\u001b[39m \u001b[38;5;28mself\u001b[39m.check_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[32m   1099\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1100\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     result = _VF.gru(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m._flat_weights, \u001b[38;5;28mself\u001b[39m.bias,\n\u001b[32m   1104\u001b[39m                      \u001b[38;5;28mself\u001b[39m.num_layers, \u001b[38;5;28mself\u001b[39m.dropout, \u001b[38;5;28mself\u001b[39m.training, \u001b[38;5;28mself\u001b[39m.bidirectional)\n",
      "\u001b[31mRuntimeError\u001b[39m: Input and hidden tensors are not at the same device, found input tensor at cpu and hidden tensor at cuda:0"
     ]
    }
   ],
   "source": [
    "# Create the MotorNet environment\n",
    "effector = mn.effector.RigidTendonArm26(muscle=mn.muscle.MujocoHillMuscle())\n",
    "env = mn.environment.RandomTargetReach(effector=effector, max_ep_duration=5.)\n",
    "\n",
    "# 2. Wrap it\n",
    "wrapped_env = MotorNetBatchLoadWrapper(env)\n",
    "device = th.device(\"cpu\")\n",
    "trained_model.policy.to(device)\n",
    "evaluate_pretrained(trained_model.policy, wrapped_env, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e8579f",
   "metadata": {},
   "source": [
    "# The one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6df10b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\" if th.cuda.is_available() else \"cpu\"\n",
    "n_batch = 1000\n",
    "batch_size = 128\n",
    "total_timesteps = n_batch * batch_size\n",
    "\n",
    "from environment import *\n",
    "from utils import *\n",
    "    \n",
    "arm = Arm('arm26')\n",
    "env = CustomTargetReach(\n",
    "    effector=arm.effector, \n",
    "    obs_noise=0.0,\n",
    "    proprioception_noise=0.0,\n",
    "    vision_noise=0.0,\n",
    "    action_noise=0.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "393782da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(th.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, device, sigma=0.1):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = 1\n",
    "        self.sigma = sigma\n",
    "        self.noise = th.zeros(output_dim, device = device)\n",
    "        self.timestep_counter = 0\n",
    "        self.resample_threshold = np.random.randint(16, 25)\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, 1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Apply custom initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            \n",
    "            if \"gru\" in name:\n",
    "                if \"weight_ih\" in name:\n",
    "                    nn.init.orthogonal_(param)\n",
    "                elif \"weight_hh\" in name:\n",
    "                    nn.init.orthogonal_(param)\n",
    "                elif \"bias_ih\" in name:\n",
    "                    nn.init.zeros_(param)\n",
    "                elif \"bias_hh\" in name:\n",
    "                    nn.init.zeros_(param)\n",
    "            elif \"fc\" in name:\n",
    "                if \"weight\" in name:\n",
    "                    nn.init.orthogonal_(param)\n",
    "                elif \"bias\" in name:\n",
    "                    nn.init.constant_(param, -10.0)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected parameter: {name}\")\n",
    "        \n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, h0):\n",
    "        y, h = self.gru(x.unsqueeze(1), h0)  \n",
    "        u = self.sigmoid(self.fc(y.squeeze(1))) \n",
    "\n",
    "        # Apply periodic Gaussian noise\n",
    "        self.timestep_counter += 1\n",
    "        if self.timestep_counter >= self.resample_threshold:\n",
    "            self.resample_noise()\n",
    "\n",
    "        return u + self.noise, h\n",
    "    \n",
    "    def resample_noise(self):\n",
    "\n",
    "        self.noise = th.randn_like(self.noise) * self.sigma\n",
    "        self.timestep_counter = 0\n",
    "        self.resample_threshold = np.random.randint(16, 25)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(self.device)\n",
    "        return hidden\n",
    "    \n",
    "class Critic(th.nn.Module):\n",
    "    def __init__(self, input_size, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Define network layers properly\n",
    "        self.fc1 = nn.Linear(input_size, 100)\n",
    "        self.fc2 = nn.Linear(100, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Apply orthogonal initialization\n",
    "        self._initialize_weights()\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Apply orthogonal initialization with gain=1 and bias=0.\"\"\"\n",
    "        for layer in [self.fc1, self.fc2, self.fc3]:\n",
    "            nn.init.orthogonal_(layer.weight, gain=1)  # Orthogonal init with gain=1\n",
    "            nn.init.zeros_(layer.bias)  # Bias = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        x = self.tanh(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb25780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyExtractor(nn.Module):\n",
    "    def __init__(self, latent_dim_pi: int, latent_dim_vf: int):\n",
    "        super().__init__()\n",
    "        # SB3 only needs these for sizing:\n",
    "        self.latent_dim_pi = latent_dim_pi\n",
    "        self.latent_dim_vf = latent_dim_vf\n",
    "\n",
    "    def forward(self, features):\n",
    "        # SB3 never actually calls it during inference for MLP policies,\n",
    "        # but it expects two outputs.\n",
    "        return features, features\n",
    "    \n",
    "class StableNetwork(ActorCriticPolicy):\n",
    "\n",
    "    def __init__(self, observation_space, action_space, lr_schedule, **kwargs):\n",
    "        device = kwargs.get('device', 'cpu')\n",
    "        kwargs.pop('device')\n",
    "        self.hidden_dim = 64\n",
    "        self.hidden_states = None\n",
    "        super().__init__(observation_space, action_space, lr_schedule, **kwargs)\n",
    "\n",
    "        self.actor = Policy(self.observation_space.shape[0], self.hidden_dim, self.action_space.shape[0], device)\n",
    "        self.critic = Critic(self.observation_space.shape[0], device)\n",
    "        self.to(device)\n",
    "        self.sigma = 0.01\n",
    "\n",
    "    def _build_mlp_extractor(self):\n",
    "        # Replace the old Dummy with our Module‐based one:\n",
    "        self.mlp_extractor = DummyExtractor(\n",
    "            latent_dim_pi=self.hidden_dim,\n",
    "            latent_dim_vf=self.hidden_dim,\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, deterministic=False):\n",
    "        \"\"\"\n",
    "        Given observations, returns:\n",
    "          - actions  (batch, act_dim)\n",
    "          - values   (batch, 1)\n",
    "          - log_prob (batch,)\n",
    "        \"\"\"\n",
    "        # ensure obs on correct device\n",
    "        obs = obs.to(self.device)\n",
    "\n",
    "        # init hidden if first call\n",
    "        batch_size = obs.shape[0]\n",
    "        if self.hidden_states is None:\n",
    "            self.hidden_states = self.actor.init_hidden(batch_size)\n",
    "\n",
    "        # actor forward\n",
    "        mean_actions, new_hidden = self.actor(obs, self.hidden_states)\n",
    "        self.hidden_states = new_hidden.detach()  # detach BPTT\n",
    "\n",
    "        # critic forward\n",
    "        values = self.critic(obs)\n",
    "\n",
    "        # build distribution\n",
    "        log_std = th.tensor(self.sigma, device=self.device).expand_as(mean_actions)\n",
    "        dist = DiagGaussianDistribution(self.action_space.shape[0])\n",
    "        dist = dist.proba_distribution(mean_actions, log_std)\n",
    "\n",
    "        # sample or mode\n",
    "        actions = dist.mode() if deterministic else dist.sample()\n",
    "        log_prob = dist.log_prob(actions)\n",
    "        \n",
    "        return actions, values, log_prob\n",
    "    \n",
    "    def evaluate_actions(self, obs, actions, hidden_states=None):\n",
    "        \"\"\"\n",
    "        Used by SB3 to compute loss:\n",
    "          - returns values, log_prob(actions), entropy\n",
    "        \"\"\"\n",
    "        obs = obs.to(self.device)\n",
    "        batch_size = obs.shape[0]\n",
    "\n",
    "         # Handle GRU hidden state properly\n",
    "        if hidden_states is not None:\n",
    "            h = hidden_states\n",
    "        else:\n",
    "            # fallback: make sure self.hidden_states has correct batch size\n",
    "            if self.hidden_states is None or self.hidden_states.shape[1] != batch_size:\n",
    "                h = th.zeros(1, batch_size, self.actor.hidden_dim, device=self.device)\n",
    "            else:\n",
    "                h = self.hidden_states\n",
    "\n",
    "        mean_actions, _ = self.actor(obs, h)\n",
    "        values = self.critic(obs)\n",
    "        \n",
    "        log_std = th.tensor(self.sigma, device=self.device).expand_as(mean_actions)\n",
    "        dist = DiagGaussianDistribution(self.action_space.shape[0])\n",
    "        dist = dist.proba_distribution(mean_actions, log_std)\n",
    "                \n",
    "        log_prob = dist.log_prob(actions)\n",
    "        entropy = dist.entropy()\n",
    "        return values, log_prob, entropy\n",
    "    \n",
    "    def predict_values(self, obs):\n",
    "        obs = obs.to(self.device)\n",
    "        return self.critic(obs)\n",
    "\n",
    "class LossTrackingCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.losses = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # SB3 doesn't expose loss directly, so we approximate it\n",
    "        # You could modify PPO class to store actual loss if needed\n",
    "        if \"train/policy_loss\" in self.logger.name_to_value:\n",
    "            self.losses.append({\n",
    "                'policy_loss': self.logger.name_to_value['train/policy_loss'],\n",
    "                'value_loss': self.logger.name_to_value['train/value_loss'],\n",
    "                'entropy_loss': self.logger.name_to_value['train/entropy_loss']\n",
    "            })\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        # This function is called after each policy update\n",
    "        # No real \"loss\", so you could simulate with mean reward or value loss\n",
    "        ep_rewards = self.locals[\"rollout_buffer\"].rewards\n",
    "        avg_reward = sum(ep_rewards) / len(ep_rewards)\n",
    "        self.losses.append(-avg_reward)  # Just an indicator proxy for loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a7cfe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTargetReach(mn.environment.Environment):\n",
    "    \"\"\"A custom reaching task:\n",
    "       - The effector starts at a random state.\n",
    "       - The target is drawn uniformly from the joint space (projected to Cartesian space),\n",
    "         with a 1 cm radius (r = 0.01 m).\n",
    "       - A 200 ms go-cue delay is imposed during which no movement is allowed.\n",
    "       - Episodes last for 5 seconds (max_ep_duration=5.0 s).\n",
    "       - The episode terminates early if the effector's endpoint stays within the target \n",
    "         region for at least 800 ms.\n",
    "       - The reward is given by: \n",
    "           Rₗ = - y_pos * L1_norm(xₜ - xₜ′) - y_ctrl * ( (uₜ * f / ∥f∥₂²)² ),\n",
    "         with\n",
    "           y_pos = 0 if ∥xₜ - xₜ′∥₂ < r, else 1, and\n",
    "           y_ctrl = 1 if ∥xₜ - xₜ′∥₂ < r, else 0.03.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, effector, **kwargs):\n",
    "        # Set task-specific parameters:\n",
    "        self.elapsed = 0.0              # elapsed time in the episode\n",
    "        self.distance_criteria = 0.005 \n",
    "        self.cue_delay = 0.2            # 200 ms no-move phase\n",
    "        # We assume the environment's dt is defined (e.g. dt = 0.02 s)\n",
    "        self.dt = kwargs.pop(\"dt\", 0.02)  # default timestep\n",
    "        self.batch_size = kwargs.pop(\"batch_size\", 128)\n",
    "        self.hold_threshold = 0.5       # 500 ms hold threshold (in seconds)\n",
    "        self.hold_time = th.zeros(self.batch_size, device=self.device)            # duration the effector is continuously within target\n",
    "\n",
    "        # Pass max_ep_duration to parent (5 seconds)\n",
    "        kwargs.setdefault(\"max_ep_duration\", 5.0)\n",
    "        super().__init__(effector, **kwargs)\n",
    "        self.__name__ = \"RandomTargetReach\"\n",
    "    \n",
    "    def reset(self, *, seed: int | None = None, options: dict | None = None) -> tuple:\n",
    "        \"\"\"\n",
    "        Reset the environment:\n",
    "          - Draw a random target joint state.\n",
    "          - Define a start joint state that is 1 cm away from the target (by adding small random noise).\n",
    "          - Convert joint states to Cartesian coordinates to set the target.\n",
    "          - Reset internal timers and observation buffers.\n",
    "        \"\"\"\n",
    "        self._set_generator(seed)  # set PRNG seeds\n",
    "        options = {} if options is None else options\n",
    "        batch_size = options.get(\"batch_size\", 1)\n",
    "        deterministic: bool = options.get('deterministic', False)\n",
    "\n",
    "        # 1. Sample random start joint state from the full joint space.\n",
    "        q_target = self.effector.draw_random_uniform_states(batch_size)  # shape: (batch_size, n_joints)\n",
    "        q_start = self.effector.draw_random_uniform_states(batch_size)\n",
    "\n",
    "        obs, info = super().reset(seed=seed, options=options)\n",
    "        \n",
    "        # 4. Set the goal.\n",
    "        cart_goal = self.joint2cartesian(q_target)\n",
    "        self.goal = cart_goal\n",
    "        info[\"goal\"] = cart_goal if self.differentiable else self.detach(cart_goal)\n",
    "        \n",
    "        # 6. Reset internal timers.\n",
    "        self.elapsed = 0.0\n",
    "        self.hold_time = th.zeros(batch_size, device=self.device)\n",
    "\n",
    "        # 7. Initialize observation buffers.\n",
    "        action = th.zeros((batch_size, self.muscle.n_muscles)).to(self.device)\n",
    "        self.obs_buffer[\"proprioception\"] = [self.get_proprioception()] * len(self.obs_buffer[\"proprioception\"])\n",
    "        self.obs_buffer[\"vision\"] = [self.get_vision()] * len(self.obs_buffer[\"vision\"])\n",
    "        self.obs_buffer[\"action\"] = [action] * self.action_frame_stacking\n",
    "\n",
    "        # 8. Get the initial observation.\n",
    "        obs = self.get_obs(deterministic=deterministic)\n",
    "        info.update({\n",
    "        \"states\": self.states,\n",
    "        \"action\": action,\n",
    "        \"noisy action\": action,  # no noise at reset\n",
    "        })\n",
    "        return obs, info\n",
    "\n",
    "    def apply_noise(self, loc, noise):\n",
    "        \"\"\"\n",
    "        Override the default noise application to disable noise.\n",
    "        This method returns the input `loc` unchanged.\n",
    "        \"\"\"\n",
    "        return loc\n",
    "    \n",
    "    def step(self, action: th.Tensor, deterministic: bool = False) -> tuple:\n",
    "        \"\"\"\n",
    "        Perform one simulation step.\n",
    "        \n",
    "        - During the first 200ms (cue_delay), actions are suppressed (set to zero) so that the effector stays\n",
    "          at the starting position.\n",
    "        - After stepping the environment, update the elapsed time.\n",
    "        - Track the time the effector's endpoint (fingertip) remains within the target.\n",
    "        - Terminate early if the hold time exceeds the threshold (800ms).\n",
    "        - Compute the reward according to:\n",
    "             Rₗ = - y_pos * L1_norm(xₜ - xₜ′) - y_ctrl * ( (uₜ * f / ∥f∥₂²)² ),\n",
    "          where:\n",
    "             y_pos = 0 if ∥xₜ - xₜ′∥₂ < r, else 1\n",
    "             y_ctrl = 1 if ∥xₜ - xₜ′∥₂ < r, else 0.03.\n",
    "        \"\"\"\n",
    "        # Freeze actions during the cue delay.\n",
    "        if self.elapsed < self.cue_delay:\n",
    "            # Handle both numpy arrays and torch tensors\n",
    "            if isinstance(action, np.ndarray):\n",
    "                action = np.zeros_like(action)\n",
    "            elif isinstance(action, th.Tensor):\n",
    "                action = th.zeros_like(action)\n",
    "        \n",
    "        # Step the simulation using the parent method.\n",
    "        obs, _, terminated, truncated, info = super().step(action, deterministic=deterministic)\n",
    "        action = th.tensor(action, device=self.device)\n",
    "        self.elapsed += self.dt\n",
    "        \n",
    "        # Compute the distance between the fingertip and the goal.\n",
    "        # Assume the effector state info includes \"states\" with key \"fingertip\".\n",
    "        fingertip = info[\"states\"][\"fingertip\"]\n",
    "        dist = th.norm(fingertip - self.goal[:, :2], dim=-1)  # L2 norm per batch\n",
    "        # Update hold time: increment where distance < threshold, reset otherwise\n",
    "        in_target = dist < self.distance_criteria\n",
    "        self.hold_time = th.where(in_target, self.hold_time + self.dt, th.zeros_like(self.hold_time))\n",
    "        \n",
    "        #Termination conditions\n",
    "        successful_terminations = self.hold_time >= self.hold_threshold\n",
    "        unsuccessful_terminations = th.full_like(\n",
    "            successful_terminations,\n",
    "            self.elapsed >= self.max_ep_duration,\n",
    "            dtype=th.bool,\n",
    "            device=successful_terminations.device\n",
    ")\n",
    "        # Combine conditions (batched OR)\n",
    "        terminated = successful_terminations | unsuccessful_terminations\n",
    "        # Truncate only if time limit is reached (Gym convention)\n",
    "        truncated = unsuccessful_terminations #th.full_like(terminated, terminated_due_to_time, dtype=th.bool)\n",
    "        \n",
    "        # Compute reward:\n",
    "        # Compute reward components\n",
    "        y_pos = th.where(in_target, th.tensor(0.0, device=action.device), th.tensor(1.0, device=action.device))\n",
    "        y_ctrl = th.where(in_target, th.tensor(1.0, device=action.device), th.tensor(0.03, device=action.device))\n",
    "        pos_error = th.sum(th.abs(fingertip - self.goal[:, :2]), dim=-1)\n",
    "        \n",
    "        # For the control term, assume f (maximum isometric contraction vector) is provided by the effector,\n",
    "        # or use ones as a default. Adjust normalization as needed.\n",
    "        f = th.tensor(self.effector.tobuild__muscle['max_isometric_force'], dtype=th.float32)\n",
    "        norm_f_squared = th.norm(f.clone().detach(), p=2) ** 2\n",
    "        f_expanded = f.expand_as(action)\n",
    "        # Compute the control term: square of (uₜ * f / norm_f_squared)\n",
    "        ctrl_term = th.sum((action * ((f_expanded) / norm_f_squared) ** 2), dim=-1)\n",
    "        \n",
    "        # Compute reward as described:\n",
    "        reward = - y_pos * pos_error - y_ctrl * ctrl_term\n",
    "        reward = reward.unsqueeze(-1)  # ensure shape is (batch_size, 1)\n",
    "        #example: reward components: tensor([0.6731, 0.7756, 0.3219, 0.4017, 0.8585, 0.6772, 0.0000, 0.8300]) and ctrl_term: tensor([-0.0016,  0.0012, -0.0026,  0.0002, -0.0034, -0.0012, -0.0427, -0.0003])\n",
    "        # Optionally, add reward components to info for debugging:\n",
    "        info[\"reward_components\"] = {\"pos_error\": pos_error, \"ctrl_term\": ctrl_term}\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885d9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m callback = LossTrackingCallback(verbose=\u001b[32m1\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    307\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    308\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    313\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    314\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m     continue_training = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[32m    303\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:195\u001b[39m, in \u001b[36mOnPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[39m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    191\u001b[39m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[32m    192\u001b[39m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[32m    193\u001b[39m         clipped_actions = np.clip(actions, \u001b[38;5;28mself\u001b[39m.action_space.low, \u001b[38;5;28mself\u001b[39m.action_space.high)\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m new_obs, rewards, dones, infos = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.num_timesteps += env.num_envs\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[39m, in \u001b[36mVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    200\u001b[39m \u001b[33;03mStep the environments with the given action\u001b[39;00m\n\u001b[32m    201\u001b[39m \n\u001b[32m    202\u001b[39m \u001b[33;03m:param actions: the action\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[38;5;28mself\u001b[39m.step_async(actions)\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:72\u001b[39m, in \u001b[36mDummyVecEnv.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     70\u001b[39m         obs, \u001b[38;5;28mself\u001b[39m.reset_infos[env_idx] = \u001b[38;5;28mself\u001b[39m.envs[env_idx].reset()\n\u001b[32m     71\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_obs(env_idx, obs)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._obs_from_buf(), np.copy(\u001b[38;5;28mself\u001b[39m.buf_rews), np.copy(\u001b[38;5;28mself\u001b[39m.buf_dones), \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuf_infos\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\copy.py:136\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    134\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\copy.py:196\u001b[39m, in \u001b[36m_deepcopy_list\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    194\u001b[39m append = y.append\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     append(\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\copy.py:136\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    134\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\copy.py:221\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    219\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\copy.py:136\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    134\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\copy.py:221\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    219\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\copy.py:143\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    141\u001b[39m copier = \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33m__deepcopy__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    145\u001b[39m     reductor = dispatch_table.get(\u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\BrainRL\\Lib\\site-packages\\torch\\_tensor.py:182\u001b[39m, in \u001b[36mTensor.__deepcopy__\u001b[39m\u001b[34m(self, memo)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensor) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    175\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe default implementation of __deepcopy__() for non-wrapper subclasses \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    176\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33monly works for subclass types that implement new_empty() and for which \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    180\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man instance of a different type.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[43mnew_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_storage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_offset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_conj():\n\u001b[32m    186\u001b[39m     new_tensor = new_tensor.conj_physical()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "env = CustomTargetReach(\n",
    "    effector=arm.effector, \n",
    "    obs_noise=0.0,\n",
    "    proprioception_noise=0.0,\n",
    "    vision_noise=0.0,\n",
    "    action_noise=0.0,\n",
    "    )\n",
    "\n",
    "model = PPO(\n",
    "    StableNetwork, env, \n",
    "    learning_rate=3e-4,\n",
    "    n_steps=total_timesteps,  # Adjust based on your environment's requirements\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    verbose=1,\n",
    "    policy_kwargs={\n",
    "        'device': DEVICE\n",
    "    }\n",
    ")\n",
    "loss_callback = LossTrackingCallback(verbose=1)\n",
    "progress_bar_callback = ProgressBarCallback()\n",
    "callbacks = [\n",
    "    loss_callback,\n",
    "    progress_bar_callback,\n",
    "]\n",
    "# Train the model\n",
    "model.learn(total_timesteps=100, callback=callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9318a90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rollouts(model, env, n_trials=5):\n",
    "    \"\"\"Plot trajectories from multiple rollouts of the trained policy.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        positions = []\n",
    "        goals = []\n",
    "        \n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Extract relevant position information - adjust based on your environment's obs structure\n",
    "            positions.append(obs['proprioception'][:2])  # First 2 elements as position\n",
    "            goals.append(obs['goal'][:2])  # First 2 elements as goal\n",
    "            \n",
    "        positions = np.array(positions)\n",
    "        goals = np.array(goals)\n",
    "        \n",
    "        # Plot trajectory\n",
    "        plt.plot(positions[:, 0], positions[:, 1], alpha=0.5, label=f'Trial {trial+1}')\n",
    "        plt.scatter(goals[-1, 0], goals[-1, 1], marker='*', s=100, c='red')\n",
    "\n",
    "    plt.title(f'Policy Trajectories ({n_trials} Trials)')\n",
    "    plt.xlabel('X Position')\n",
    "    plt.ylabel('Y Position')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_losses(loss_history):\n",
    "    \"\"\"Plot training losses from the callback.\"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # Unpack loss components - adjust based on your callback's storage format\n",
    "    policy_loss = [x['policy_loss'] for x in loss_history]\n",
    "    value_loss = [x['value_loss'] for x in loss_history]\n",
    "    entropy_loss = [x['entropy_loss'] for x in loss_history]\n",
    "    \n",
    "    x = np.arange(len(loss_history))\n",
    "    \n",
    "    plt.plot(x, policy_loss, label='Policy Loss')\n",
    "    plt.plot(x, value_loss, label='Value Loss')\n",
    "    plt.plot(x, entropy_loss, label='Entropy Loss')\n",
    "    \n",
    "    plt.title('Training Loss Curves')\n",
    "    plt.xlabel('Update Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d3765",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rollouts(model, env, n_trials=5)\n",
    "plot_losses(callback.losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BrainRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
